//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_89
.address_size 64

	// .globl	_rms_layernorm_forward
.extern .shared .align 16 .b8 global_smem[];
.global .align 1 .b8 _$_str[11] = {95, 95, 67, 85, 68, 65, 95, 70, 84, 90};

.visible .entry _rms_layernorm_forward(
	.param .u64 _rms_layernorm_forward_param_0,
	.param .u32 _rms_layernorm_forward_param_1,
	.param .u64 _rms_layernorm_forward_param_2,
	.param .u32 _rms_layernorm_forward_param_3,
	.param .u64 _rms_layernorm_forward_param_4,
	.param .u64 _rms_layernorm_forward_param_5,
	.param .u32 _rms_layernorm_forward_param_6,
	.param .f32 _rms_layernorm_forward_param_7
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<41>;
	.reg .b32 	%r<81>;
	.reg .f32 	%f<47>;
	.reg .b64 	%rd<15>;
	.loc	1 22 0
$L__func_begin0:
	.loc	1 22 0

	ld.param.u64 	%rd5, [_rms_layernorm_forward_param_0];
	ld.param.u32 	%r48, [_rms_layernorm_forward_param_1];
$L__tmp0:
	.loc	1 35 28
	// begin inline asm
	mov.u32 %r1, %ctaid.x;
	// end inline asm
	ld.param.u64 	%rd6, [_rms_layernorm_forward_param_2];
	.loc	1 36 31
	mov.u32 	%r49, %tid.x;
	and.b32  	%r50, %r49, 31;
	ld.param.u32 	%r51, [_rms_layernorm_forward_param_3];
	ld.param.u64 	%rd7, [_rms_layernorm_forward_param_4];
	shl.b32 	%r52, %r49, 3;
	ld.param.u64 	%rd8, [_rms_layernorm_forward_param_5];
	and.b32  	%r53, %r52, 2040;
	ld.param.u32 	%r54, [_rms_layernorm_forward_param_6];
	.loc	1 37 25
	setp.lt.s32 	%p1, %r53, %r54;
	ld.param.f32 	%f1, [_rms_layernorm_forward_param_7];
	.loc	1 40 19
	mul.lo.s32 	%r55, %r1, %r51;
	.loc	1 40 9
	mul.wide.s32 	%rd9, %r55, 2;
	add.s64 	%rd10, %rd6, %rd9;
	.loc	1 43 24
	mul.wide.u32 	%rd11, %r53, 2;
	add.s64 	%rd1, %rd10, %rd11;
	mov.b32 	%r6, 0;
	.loc	1 43 20
	// begin inline asm
	mov.u32 %r2, 0x0;
	mov.u32 %r3, 0x0;
	mov.u32 %r4, 0x0;
	mov.u32 %r5, 0x0;
	@%p1 ld.global.v4.b32 { %r2, %r3, %r4, %r5 }, [ %rd1 + 0 ];
	@!%p1 mov.u32 %r2, %r6;
	@!%p1 mov.u32 %r3, %r6;
	@!%p1 mov.u32 %r4, %r6;
	@!%p1 mov.u32 %r5, %r6;
	// end inline asm
	cvt.u16.u32 	%rs1, %r2;
	{ .reg .b16 tmp; mov.b32 {tmp, %rs2}, %r2; }
	cvt.u16.u32 	%rs3, %r3;
	{ .reg .b16 tmp; mov.b32 {tmp, %rs4}, %r3; }
	cvt.u16.u32 	%rs5, %r4;
	{ .reg .b16 tmp; mov.b32 {tmp, %rs6}, %r4; }
	cvt.u16.u32 	%rs7, %r5;
	{ .reg .b16 tmp; mov.b32 {tmp, %rs8}, %r5; }
	.loc	1 43 64
	// begin inline asm
	cvt.f32.bf16 %r10, %rs1;
	// end inline asm
	mov.b32 	%f2, %r10;
	// begin inline asm
	cvt.f32.bf16 %r11, %rs2;
	// end inline asm
	mov.b32 	%f3, %r11;
	// begin inline asm
	cvt.f32.bf16 %r12, %rs3;
	// end inline asm
	mov.b32 	%f4, %r12;
	// begin inline asm
	cvt.f32.bf16 %r13, %rs4;
	// end inline asm
	mov.b32 	%f5, %r13;
	// begin inline asm
	cvt.f32.bf16 %r14, %rs5;
	// end inline asm
	mov.b32 	%f6, %r14;
	// begin inline asm
	cvt.f32.bf16 %r15, %rs6;
	// end inline asm
	mov.b32 	%f7, %r15;
	// begin inline asm
	cvt.f32.bf16 %r16, %rs7;
	// end inline asm
	mov.b32 	%f8, %r16;
	// begin inline asm
	cvt.f32.bf16 %r17, %rs8;
	// end inline asm
	mov.b32 	%f9, %r17;
	.loc	1 44 24
	add.s64 	%rd2, %rd7, %rd11;
	.loc	1 44 20
	// begin inline asm
	mov.u32 %r18, 0x0;
	mov.u32 %r19, 0x0;
	mov.u32 %r20, 0x0;
	mov.u32 %r21, 0x0;
	@%p1 ld.global.v4.b32 { %r18, %r19, %r20, %r21 }, [ %rd2 + 0 ];
	@!%p1 mov.u32 %r18, %r6;
	@!%p1 mov.u32 %r19, %r6;
	@!%p1 mov.u32 %r20, %r6;
	@!%p1 mov.u32 %r21, %r6;
	// end inline asm
	.loc	1 46 29
	mul.f32 	%f10, %f3, %f3;
$L__tmp1:
	.loc	2 256 15
	fma.rn.f32 	%f11, %f2, %f2, %f10;
	fma.rn.f32 	%f12, %f4, %f4, %f11;
	fma.rn.f32 	%f13, %f5, %f5, %f12;
	fma.rn.f32 	%f14, %f6, %f6, %f13;
	fma.rn.f32 	%f15, %f7, %f7, %f14;
	fma.rn.f32 	%f16, %f8, %f8, %f15;
	fma.rn.f32 	%f17, %f9, %f9, %f16;
	.loc	2 267 36
	mov.b32 	%r56, %f17;
	shfl.sync.bfly.b32	%r57, %r56, 16, 31, -1;
	mov.b32 	%f18, %r57;
	.loc	2 256 15
	add.f32 	%f19, %f17, %f18;
	.loc	2 267 36
	mov.b32 	%r58, %f19;
	shfl.sync.bfly.b32	%r59, %r58, 8, 31, -1;
	mov.b32 	%f20, %r59;
	.loc	2 256 15
	add.f32 	%f21, %f19, %f20;
	.loc	2 267 36
	mov.b32 	%r60, %f21;
	shfl.sync.bfly.b32	%r61, %r60, 4, 31, -1;
	mov.b32 	%f22, %r61;
	.loc	2 256 15
	add.f32 	%f23, %f21, %f22;
	.loc	2 267 36
	mov.b32 	%r62, %f23;
	shfl.sync.bfly.b32	%r63, %r62, 2, 31, -1;
	mov.b32 	%f24, %r63;
	.loc	2 256 15
	add.f32 	%f25, %f23, %f24;
	.loc	2 267 36
	mov.b32 	%r64, %f25;
	shfl.sync.bfly.b32	%r65, %r64, 1, 31, -1;
	mov.b32 	%f26, %r65;
	.loc	2 256 15
	add.f32 	%f27, %f25, %f26;
	.loc	2 267 36
	setp.eq.s32 	%p11, %r50, 0;
	shr.u32 	%r66, %r49, 3;
	and.b32  	%r67, %r66, 28;
	mov.u32 	%r68, global_smem;
	add.s32 	%r26, %r68, %r67;
	mov.b32 	%r27, %f27;
	// begin inline asm
	@%p11 st.shared.b32 [ %r26 + 0 ], %r27;
	// end inline asm
	bar.sync 	0;
	setp.lt.s32 	%p12, %r49, 8;
	shl.b32 	%r69, %r49, 2;
	add.s32 	%r29, %r68, %r69;
	// begin inline asm
	@%p12 ld.shared.b32 %r28, [ %r29 + 0 ];
	// end inline asm
	mov.b32 	%f28, %r28;
	shfl.sync.bfly.b32	%r70, %r28, 4, 31, -1;
	mov.b32 	%f29, %r70;
	.loc	2 256 15
	add.f32 	%f30, %f28, %f29;
	.loc	2 267 36
	mov.b32 	%r71, %f30;
	shfl.sync.bfly.b32	%r72, %r71, 2, 31, -1;
	mov.b32 	%f31, %r72;
	.loc	2 256 15
	add.f32 	%f32, %f30, %f31;
	.loc	2 267 36
	mov.b32 	%r73, %f32;
	shfl.sync.bfly.b32	%r74, %r73, 1, 31, -1;
	mov.b32 	%f33, %r74;
	.loc	2 256 15
	add.f32 	%f34, %f32, %f33;
	.loc	2 267 36
	and.b32  	%r75, %r49, 7;
	setp.eq.s32 	%p16, %r75, 0;
	and.pred  	%p13, %p12, %p16;
	mov.b32 	%r31, %f34;
	// begin inline asm
	@%p13 st.shared.b32 [ %r29 + 0 ], %r31;
	// end inline asm
	bar.sync 	0;
$L__tmp2:
	.loc	1 46 48
	cvt.rn.f32.s32 	%f35, %r54;
	mov.b32 	%r34, %f35;
	ld.shared.u32 	%r33, [global_smem];
	// begin inline asm
	div.full.f32 %r32, %r33, %r34;
	// end inline asm
	mov.b32 	%f36, %r32;
	.loc	1 47 38
	add.f32 	%f37, %f36, %f1;
	.loc	1 47 28
	rsqrt.approx.ftz.f32 	%f38, %f37;
	.loc	1 44 20
	{ .reg .b16 tmp; mov.b32 {tmp, %rs40}, %r21; }
	cvt.u16.u32 	%rs37, %r21;
	{ .reg .b16 tmp; mov.b32 {tmp, %rs34}, %r20; }
	cvt.u16.u32 	%rs31, %r20;
	{ .reg .b16 tmp; mov.b32 {tmp, %rs28}, %r19; }
	cvt.u16.u32 	%rs25, %r19;
	{ .reg .b16 tmp; mov.b32 {tmp, %rs22}, %r18; }
	cvt.u16.u32 	%rs19, %r18;
	.loc	1 41 9
	mul.wide.s32 	%rd12, %r1, 4;
	add.s64 	%rd3, %rd8, %rd12;
	.loc	1 39 19
	mul.lo.s32 	%r76, %r1, %r48;
	.loc	1 39 9
	mul.wide.s32 	%rd13, %r76, 2;
	add.s64 	%rd14, %rd5, %rd13;
	.loc	1 48 16
	setp.eq.s32 	%p14, %r49, 0;
	mov.b32 	%r35, %f38;
	// begin inline asm
	@%p14 st.global.b32 [ %rd3 + 0 ], { %r35 };
	// end inline asm
	.loc	1 49 21
	mul.f32 	%f39, %f2, %f38;
	mul.f32 	%f40, %f3, %f38;
	mul.f32 	%f41, %f4, %f38;
	mul.f32 	%f42, %f5, %f38;
	mul.f32 	%f43, %f6, %f38;
	mul.f32 	%f44, %f7, %f38;
	mul.f32 	%f45, %f8, %f38;
	mul.f32 	%f46, %f9, %f38;
	.loc	1 50 23
	mov.b32 	%r36, %f39;
	// begin inline asm
	cvt.rn.bf16.f32 %rs18, %r36;
	// end inline asm
	mov.b32 	%r37, %f40;
	// begin inline asm
	cvt.rn.bf16.f32 %rs21, %r37;
	// end inline asm
	mov.b32 	%r38, %f41;
	// begin inline asm
	cvt.rn.bf16.f32 %rs24, %r38;
	// end inline asm
	mov.b32 	%r39, %f42;
	// begin inline asm
	cvt.rn.bf16.f32 %rs27, %r39;
	// end inline asm
	mov.b32 	%r40, %f43;
	// begin inline asm
	cvt.rn.bf16.f32 %rs30, %r40;
	// end inline asm
	mov.b32 	%r41, %f44;
	// begin inline asm
	cvt.rn.bf16.f32 %rs33, %r41;
	// end inline asm
	mov.b32 	%r42, %f45;
	// begin inline asm
	cvt.rn.bf16.f32 %rs36, %r42;
	// end inline asm
	mov.b32 	%r43, %f46;
	// begin inline asm
	cvt.rn.bf16.f32 %rs39, %r43;
	// end inline asm
	.loc	1 51 22
	// begin inline asm
	 { .reg .b16 c;        
    mov.b16 c, 0x8000U; 
    fma.rn.bf16 %rs17, %rs18, %rs19, c; } 

	// end inline asm
	// begin inline asm
	 { .reg .b16 c;        
    mov.b16 c, 0x8000U; 
    fma.rn.bf16 %rs20, %rs21, %rs22, c; } 

	// end inline asm
	// begin inline asm
	 { .reg .b16 c;        
    mov.b16 c, 0x8000U; 
    fma.rn.bf16 %rs23, %rs24, %rs25, c; } 

	// end inline asm
	// begin inline asm
	 { .reg .b16 c;        
    mov.b16 c, 0x8000U; 
    fma.rn.bf16 %rs26, %rs27, %rs28, c; } 

	// end inline asm
	// begin inline asm
	 { .reg .b16 c;        
    mov.b16 c, 0x8000U; 
    fma.rn.bf16 %rs29, %rs30, %rs31, c; } 

	// end inline asm
	// begin inline asm
	 { .reg .b16 c;        
    mov.b16 c, 0x8000U; 
    fma.rn.bf16 %rs32, %rs33, %rs34, c; } 

	// end inline asm
	// begin inline asm
	 { .reg .b16 c;        
    mov.b16 c, 0x8000U; 
    fma.rn.bf16 %rs35, %rs36, %rs37, c; } 

	// end inline asm
	// begin inline asm
	 { .reg .b16 c;        
    mov.b16 c, 0x8000U; 
    fma.rn.bf16 %rs38, %rs39, %rs40, c; } 

	// end inline asm
	.loc	1 52 17
	add.s64 	%rd4, %rd14, %rd11;
	.loc	1 52 30
	mov.b32 	%r77, {%rs17, %rs20};
	mov.b32 	%r78, {%rs23, %rs26};
	mov.b32 	%r79, {%rs29, %rs32};
	mov.b32 	%r80, {%rs35, %rs38};
	// begin inline asm
	@%p1 st.global.v4.b32 [ %rd4 + 0 ], { %r77, %r78, %r79, %r80 };
	// end inline asm
	.loc	1 52 4
	ret;
$L__tmp3:
$L__func_end0:

}
	.file	1 "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/unsloth/kernels/rms_layernorm.py"
	.file	2 "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/triton/language/standard.py"
	.section	.debug_abbrev
	{
.b8 1
.b8 17
.b8 1
.b8 37
.b8 8
.b8 19
.b8 5
.b8 3
.b8 8
.b8 16
.b8 6
.b8 27
.b8 8
.b8 17
.b8 1
.b8 18
.b8 1
.b8 0
.b8 0
.b8 2
.b8 46
.b8 0
.b8 3
.b8 8
.b8 32
.b8 11
.b8 0
.b8 0
.b8 3
.b8 46
.b8 1
.b8 17
.b8 1
.b8 18
.b8 1
.b8 49
.b8 19
.b8 0
.b8 0
.b8 4
.b8 29
.b8 0
.b8 49
.b8 19
.b8 17
.b8 1
.b8 18
.b8 1
.b8 88
.b8 11
.b8 89
.b8 11
.b8 87
.b8 11
.b8 0
.b8 0
.b8 0
	}
	.section	.debug_info
	{
.b32 209
.b8 2
.b8 0
.b32 .debug_abbrev
.b8 8
.b8 1
.b8 116
.b8 114
.b8 105
.b8 116
.b8 111
.b8 110
.b8 0
.b8 2
.b8 0
.b8 114
.b8 109
.b8 115
.b8 95
.b8 108
.b8 97
.b8 121
.b8 101
.b8 114
.b8 110
.b8 111
.b8 114
.b8 109
.b8 46
.b8 112
.b8 121
.b8 0
.b32 .debug_line
.b8 47
.b8 104
.b8 111
.b8 109
.b8 101
.b8 47
.b8 122
.b8 101
.b8 117
.b8 115
.b8 47
.b8 109
.b8 105
.b8 110
.b8 105
.b8 99
.b8 111
.b8 110
.b8 100
.b8 97
.b8 51
.b8 47
.b8 101
.b8 110
.b8 118
.b8 115
.b8 47
.b8 99
.b8 108
.b8 111
.b8 117
.b8 100
.b8 115
.b8 112
.b8 97
.b8 99
.b8 101
.b8 47
.b8 108
.b8 105
.b8 98
.b8 47
.b8 112
.b8 121
.b8 116
.b8 104
.b8 111
.b8 110
.b8 51
.b8 46
.b8 49
.b8 48
.b8 47
.b8 115
.b8 105
.b8 116
.b8 101
.b8 45
.b8 112
.b8 97
.b8 99
.b8 107
.b8 97
.b8 103
.b8 101
.b8 115
.b8 47
.b8 117
.b8 110
.b8 115
.b8 108
.b8 111
.b8 116
.b8 104
.b8 47
.b8 107
.b8 101
.b8 114
.b8 110
.b8 101
.b8 108
.b8 115
.b8 0
.b64 $L__func_begin0
.b64 $L__func_end0
.b8 2
.b8 95
.b8 114
.b8 109
.b8 115
.b8 95
.b8 108
.b8 97
.b8 121
.b8 101
.b8 114
.b8 110
.b8 111
.b8 114
.b8 109
.b8 95
.b8 102
.b8 111
.b8 114
.b8 119
.b8 97
.b8 114
.b8 100
.b8 0
.b8 1
.b8 3
.b64 $L__func_begin0
.b64 $L__func_end0
.b32 141
.b8 4
.b32 141
.b64 $L__tmp1
.b64 $L__tmp2
.b8 1
.b8 46
.b8 21
.b8 0
.b8 0
	}
	.section	.debug_loc	{	}
